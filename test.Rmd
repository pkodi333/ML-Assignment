---
title: 'STA 380, Part 2: Exercises'
author: "Prashanti Kodi"
date: "8/16/2021"
output: pdf_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(rmarkdown)
library(markdown)
library(tinytex)

```

# Visual story telling part 1: green buildings

An Austin real-estate developer is interested in the possible economic impact of "going green" in her latest project: a new 15-story mixed-use building on East Cesar Chavez, just across I-35 from downtown. Will investing in a green building be worth it, from an economic perspective? The baseline construction costs are $100 million, with a 5% expected premium for green certification.

The developer has had someone on her staff, who's been described to her as a "total Excel guru from his undergrad statistics course," run some numbers on this data set and make a preliminary recommendation. Here's how this person described his process.

*I began by cleaning the data a little bit. In particular, I noticed that a handful of the buildings in the data set had very low occupancy rates (less than 10% of available space occupied). I decided to remove these buildings from consideration, on the theory that these buildings might have something weird going on with them, and could potentially distort the analysis. Once I scrubbed these low-occupancy buildings from the data set, I looked at the green buildings and non-green buildings separately. The median market rent in the non-green buildings was $25 per square foot per year, while the median market rent in the green buildings was $27.60 per square foot per year: about $2.60 more per square foot. (I used the median rather than the mean, because there were still some outliers in the data, and the median is a lot more robust to outliers.) Because our building would be 250,000 square feet, this would translate into an additional $250000 x 2.6 = $650000 of extra revenue per year if we build the green building.*

Do you agree with the conclusions of her on-staff stats guru? Tell your story mainly in pictures, with appropriate introductory and supporting text.

A. First, Let us visualize the data using green buildings as a factor.

````{r , echo=FALSE}
library (readr)
library (ggplot2)

#access url and read the data

urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv"

mydata<-read_csv(url(urlfile),col_types = cols())


p= ggplot(mydata, aes(x = size, y = Rent)) +
geom_point(aes(color = factor(green_rating)))
p + labs(title = "Green buildings: Size VS Rent")


p1 = ggplot(mydata, aes(x = age, y = Rent)) +
    geom_point(aes(color = factor(green_rating)))
p1 + labs(title = "Green buildings: Age VS Rent")


p2 = ggplot(mydata, aes(x = cluster_rent, y = Rent)) +
    geom_point(aes(color = factor(green_rating)))
p2 + labs(title = "Green buildings: Cluster Rent VS Rent")

ggplot(data=mydata) + 
  geom_point(mapping=aes(x=age, y=Rent, colour=class_a))+
  labs(x="Age", y='Rent', title = 'Class A: Age VS Rent',
       color='Class A building')


ggplot(data=mydata) + 
  geom_point(mapping=aes(x=size, y=Rent, colour=class_a))+
  labs(x="Size", y='Rent', title = 'Class A: Size VS Rent',
       color='Class A building')

````
**We can make the following observations from above:**

1. Rent is correlated with size.
2. Rent isn't very correlated with age.
3. Rent and cluster rent are correlated.
4. Class A buildings are younger and rent is higher.
5. Size is bigger for the class A buildings.

````{r , echo=FALSE}
library (readr)
library (ggplot2)

#access url and read the data

urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv"

mydata<-read_csv(url(urlfile),col_types = cols())
g = ggplot(mydata, aes(x=age))
g + geom_density(aes(fill=factor(green_rating)), alpha=0.4)+
  labs(x="Age", y='Density', title = 'Distribution of age',
       fill='Green building')
g = ggplot(mydata, aes(x=size))
g + geom_density(aes(fill=factor(green_rating)), alpha=0.4)+
  labs(x="Size", y='Density', title = 'Distribution of size',
       fill='Green building')
````
**Observations:**  

1. The green buildings are mostly younger than the non green ones
2. Most of the buildings are smaller in size.

````{r , echo=FALSE}
library (readr)
library (ggplot2)

#access url and read the data

urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv"

mydata<-read_csv(url(urlfile),col_types = cols())
counts <- table(mydata$green_rating, mydata$class_a)
barplot(counts, main="Number of houses by green rated and Class a",
  xlab="Class a", col=c("darkblue","red"),
  legend = rownames(counts), beside=TRUE)
````
**Observations:**

Class A buildings are higher in number in green buildings.

````{r gb, echo=FALSE}
library (readr)
library (ggplot2)

#access url and read the data

urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/greenbuildings.csv"

mydata<-read_csv(url(urlfile),col_types = cols())

mydata$age_cat <- cut(mydata$age, breaks = c(0, seq(10, 190, by = 10)), labels = 0:18,right=FALSE)
medians <- aggregate(Rent~ age_cat + green_rating, mydata, median)
ggplot(data = medians, mapping = aes(y = Rent, x = age_cat ,group = green_rating, colour=green_rating)) +
   geom_line(size=1.2) +
  labs(x="Age in 10 years", y='Median Rent', title = 'All buildings: Median rent over the years',
       fill='Green building')
# Size in 100k
mydata$size_cat <- cut(mydata$size, breaks = c(0, seq(10, 3781045, by = 100000)), labels = 0:37,right=FALSE)
medians <- aggregate(Rent ~ size_cat + green_rating, mydata, median)
ggplot(data = medians, mapping = aes(y = Rent, x = size_cat ,group = green_rating, colour=green_rating)) +
   geom_line(size=1.2) +
  labs(x="Size in 100k sq.ft", y='Median Rent', title = 'All buildings: median rent for different building sizes',
       fill='Green building')
data_non_class_a <- subset(mydata, mydata$class_a != 1)
data_non_class_a$age_cat <- cut(data_non_class_a$age, breaks = c(0, seq(10, 190, by = 10)), labels = 0:18,right=FALSE)
medians <- aggregate(Rent~ age_cat + green_rating, data_non_class_a, median)
ggplot(data = medians, mapping = aes(y = Rent, x = age_cat ,group = green_rating, colour=green_rating)) +
   geom_line(size=1.2)+
  labs(x="Age in 10 years", y='Median Rent', title = 'Non-Class A buildings: Median rent over the years',
       fill='Green building')
# Size in 100k
data_non_class_a$size_cat <- cut(data_non_class_a$size, breaks = c(0, seq(10, 3781045, by = 100000)), labels = 0:37,right=FALSE)
medians <- aggregate(Rent ~ size_cat + green_rating, data_non_class_a, median)
ggplot(data = medians, mapping = aes(y = Rent, x = size_cat ,group = green_rating, colour=green_rating)) +
   geom_line(size=1.2)+
  labs(x="Size in 100k sq.ft", y='Median Rent', title = 'Non-class A buildings: median rent for different building sizes',
       fill='Green building')
````
**Observations:**

1. Rent isn't uniform when considering age and size.


**Take-aways:**

1. Rent difference isn't uniform so, we can not calculate the returns.
2. We should use the median leasing rate of such buildings instead of the 90% rate.
3. For 250,000 sqft, green buildings rents are higher when its Class A.
4.  We don't have much information for the Class A buildings of sizes 200,000 to 300,000 sqft.

We see that the analysis by stats guru is flawed since he didn't consider all the factors that affect rent. First, he used median rent of all the buildings to calculate returns. So, he didn't use other factors like size and age in the analysis.

**Final Thoughts:**

1. Builders should invest in Class-A green buildings to yield positive returns.
2. The rent differences for the Class A buildings of sizes 200,000 to 300,000 sqft is very minimal.
3. We can expect more than 90% occupancy for Class-A green buildings. So, we can recuperate the costs for a 250k sqft building at a shorter time than calculated.

# Visual story telling part 2: flights at ABIA

Your task is to create a figure, or set of related figures, that tell an interesting story about flights into and out of Austin. Provide a clear annotation/caption for each figure, but the figure should be more or less stand-alone, in that you shouldn't need many, many paragraphs to convey its meaning. Rather, the figure together with a concise caption should speak for itself as far as possible.

````{r , echo=FALSE}
library (readr)
library (ggplot2)

urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv"

mydata1<-read_csv(url(urlfile),col_types = cols())

ggplot(data = mydata1, aes(x=ArrDelay)) + 
  geom_histogram(bins = 100, binwidth = 10) + 
  xlab('Arrival Delay') +
  ggtitle('Distribution of Arrival Delays')

ggplot(data = mydata1, aes(x=DepDelay)) + 
  geom_histogram(bins = 100, binwidth = 10) + 
  xlab('Departure Delay') +
  ggtitle('Distribution of Departure Delays')
````
**Observations:**

The delays for both arrival and departure are mostly in the bin for zero.

````{r , echo=FALSE}
library (readr)
library (ggplot2)

urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv"

mydata1<-read_csv(url(urlfile),col_types = cols())

pl <- ggplot(aes(x=DepDelay, y=ArrDelay), data=mydata1) +
  geom_point(aes(color=UniqueCarrier))
print(pl +
        ggtitle('Correlation between arrival and departure delays') +
        xlab('Departure Delay') +
        ylab('Arrival Delay'))
````
**Observations**

We see a constant slope and can not single out any carrier to be responsible for delays.

````{r air, echo=FALSE}
library (readr)
library (ggplot2)

urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv"

mydata1<-read_csv(url(urlfile),col_types = cols())

ggplot(aes(x=UniqueCarrier), data=mydata1) +
geom_bar(fill='black', position='dodge') +
ggtitle('Number of operations by Carrier') +
xlab('Carrier Name') +
ylab('Number of operations')
  
````
**Observations**

When we look at the carrier operations at the Austin Airport, we see that Southwest(WN) has the highest number of operations at around 35k. Northwest(NW) has the lowest number of operations at around 100.

````{r ap , echo=FALSE}
library (readr)
library (ggplot2)
library(FSA)

urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv"

mydata1<-read_csv(url(urlfile),col_types = cols())
Sum = Summarize(CarrierDelay ~ UniqueCarrier,
                data=mydata1)
Table = as.table(Sum$mean)

rownames(Table) = Sum$UniqueCarrier
barplot(Table,
        ylab="Mean Delay",
        xlab="Carrier")
```
**Observations**

Southwest(WN) despite having a higher number in operations, has a relatively low delay. Mesa(YV) has the highest average delay out of all the carriers.

````{r , echo=FALSE}
library (readr)
library (ggplot2)


urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv"

mydata1<-read_csv(url(urlfile),col_types = cols())

Carrier = mydata1$CarrierDelay
Weather = mydata1$WeatherDelay
NAS = mydata1$NASDelay
LateAircraft = mydata1$LateAircraftDelay
Security = mydata1$SecurityDelay
delays = data.frame(row.names = c('Carrier', 'Weather', 'NAS', 'Security', 'LateAircraft'),  
'Total'=c(sum(Carrier, na.rm=TRUE), sum(Weather, na.rm=TRUE), sum(NAS, na.rm=TRUE), sum(Security, na.rm=TRUE), sum(LateAircraft, na.rm=TRUE)))
ggplot(delays, aes(x=rownames(delays), y=Total)) +
  geom_bar(stat = 'identity') +
  ggtitle('Total delay time in mins across different delay types') +
  xlab('Type of delay') +
  ylab('Delay in minutes')

Carrier[is.na(Carrier)] <- 0
Weather[is.na(Weather)] <- 0
NAS[is.na(NAS)] <- 0
Security[is.na(Security)] <- 0
LateAircraft[is.na(LateAircraft)] <- 0
delay_count = c(sum(Carrier>0), sum(Weather>0), sum(NAS>0), sum(Security>0), sum(LateAircraft>0))
delays$delay_count <- delay_count
ggplot(delays, aes(x=rownames(delays), y=delay_count)) +
  geom_bar(stat = 'identity') +
  ggtitle('Number of delays recorded across different delay types') +
  xlab('Type of delay') +
  ylab('Number of delays')
````

**Observations**

Delays due to NAS, Late Aircraft and Carrier were frequent whilst having higher delay times.
Security was low for both number of delays and delay time.

````{r dist , echo=FALSE}
library (readr)
library (ggplot2)


urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv"

mydata1<-read_csv(url(urlfile),col_types = cols())

counts <- table(mydata1$Month)
barplot(counts, main="Month Distribution",
   xlab="Number of delays per month")

counts1 <- table(mydata1$DayOfWeek)
barplot(counts1, main="Weekly Distribution",
   xlab="Number of delays in week")

```

**Observations**

We can see the maximum delays during the months of March, June and July.
The number of delays is lower on the weekends when compared to the weekdays.

````{r , echo=FALSE , message=FALSE}
library (readr)
library (ggplot2)
library(dplyr)

urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv"

mydata1<-read_csv(url(urlfile),col_types = cols())

aus.dep <- subset(mydata1, Origin == 'AUS')
aus.arr <- subset(mydata1, Dest == 'AUS')

cat("Number of Departures-", nrow(aus.dep))
cat("\nNumber of Arrivals-", nrow(aus.arr), "\n")

mydata1$Dep_Hr <- sapply(mydata1$DepTime, function(x) x%/%100)
mydata1$CRSDep_Hr <- sapply(mydata1$CRSDepTime, function(x) x%/%100)
mydata1$Arr_Hr <- sapply(mydata1$ArrTime, function(x) x%/%100)
mydata1$CRSArr_Hr <- sapply(mydata1$CRSArrTime, function(x) x%/%100)

aus.dep <- subset(mydata1, Origin == 'AUS')
aus.arr <- subset(mydata1, Dest == 'AUS')
CRSDep_Hr = aus.dep$CRSDep_Hr
CRSArr_Hr = aus.arr$CRSArr_Hr
aus.trafficDep <- aus.dep %>%
  group_by(CRSDep_Hr) %>%
  summarise(count_actualDep = length(Year))
aus.trafficArr <- aus.arr %>%
  group_by(CRSArr_Hr) %>%
  summarise(count_actualArr = length(Year))
aus.traffic <- merge(x = aus.trafficDep, y = aus.trafficArr, by.x ='CRSDep_Hr', by.y = 'CRSArr_Hr', all = TRUE)
aus.traffic[is.na(aus.traffic)] <- 0
aus.traffic$Total_Flights <- aus.traffic$count_actualDep + aus.traffic$count_actualArr
Total_Flights = aus.traffic$Total_Flights
ggplot(aus.traffic, aes(x=CRSDep_Hr, y=Total_Flights)) +
  geom_bar(stat = 'identity',fill='#CE4150') +
  xlab('Hour of the day') +
  ylab('Flight count') +
  ggtitle('Total Flights by Hour of the day (Dep and Arr inclusive)')

```

**Observations**

11 AM to 5PM seems to be the busiest time at the Airport while there seems to be less air traffic during the early mornings till 5AM.

# Portfolio modeling

In this problem, you will construct three different portfolios of exchange-traded funds, or ETFs, and use bootstrap resampling to analyze the short-term tail risk of your portfolios. 
Construct three different possibilities for an ETF-based portfolio, each involving an allocation of your $100,000 in capital to somewhere between 3 and 10 different ETFs.
Write a report summarizing your portfolios and your VaR findings.

````{r , echo=FALSE , message=FALSE,  warning=FALSE, include=FALSE}
library(mosaic)
library(quantmod)
library(foreach)

mystocks = c("AOR","BBP","USMV","QQQ")

getSymbols(mystocks, from='2014-01-01')
for(ticker in mystocks){
  expr = paste0(ticker, "a=adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}

all_returns = cbind(ClCl(AORa),ClCl(BBPa),ClCl(USMVa),ClCl(QQQa))
all_returns = as.matrix(na.omit(all_returns))

pairs(all_returns)
```

**Background of the ETF's chosen:**

iShares Core Growth Allocation ETF (AOR) is a target-risk ETF and is very diverse. It has low risk and yields average returns.

Virtus LifeSci Biotech Products ETF (BBP) is a Health
Care sector ETF. It is one of the riskiest ETFs as 68% of the
fundâ€™s holdings are rated either Very Dangerous or Dangerous.

iShares MSCI USA Min Vol Factor ETF (USMV) is the largest low-volatile ETF on Wall Street and is considered one of the best for low-risk investors.

Invesco QQQ Trust(QQQ) is one of the largest among aggressive growth ETFs.

**Observations:**

There is a strong correlation with how the stocks perform with each other. It is sort of linear in some cases.


*Now let us build the various portfolios.*

The first portfolio is a normal portfolio in which all stocks are given equal weights.


```{r , echo=FALSE, message=FALSE}


total_wealth = 100000

sim1 = foreach(i=1:5000, .combine = rbind) %do% {
  weights = c(0.25,0.25,0.25,0.25)
  holdings = total_wealth * weights
  n_days = 20
  wealthtracker = rep(0, n_days)
  
  for(today in 1:n_days){
    return_today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings * (1 + return_today)
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    
    # Rebalancing
    holdings = total_wealth * weights
  }
  
  wealthtracker
}

#Profit/loss
mean(sim1[,n_days])
mean(sim1[,n_days] - total_wealth)

conf_5Per = confint(sim1[,n_days]- total_wealth, level = 0.90)$'5%'
cat('\nAverage return of investement after 20 days', mean(sim1[,n_days]), "\n")
cat('\n5% Value at Risk for random normal portfolio-',conf_5Per, "\n")
wealth_daywise = c()
  
for (i in 1:n_days){
    wealth_daywise[i] = mean(sim1[,i]) 
}
days = 1:n_days
df = data.frame(wealth_daywise, days)
ggplot(data=df, aes(x=days, y=wealth_daywise, group=1)) +
  geom_line(color="red")+
  geom_point() +
  xlab('Days') +
  ylab('Return of investments') + 
  ggtitle('Normal Risk Portfolio: Returns over 20 days')
```

The second portfolio is a high risk portfolio in which all volatile stocks are collectively given higher weights.

```{r , echo=FALSE, message=FALSE}
total_wealth = 100000
sim2 = foreach(i=1:5000, .combine = rbind) %do% {
  weights = c(0.02,0.85,0.03,0.1)
  holdings = total_wealth * weights
  n_days = 20
  wealthtracker = rep(0, n_days)
  
  for(today in 1:n_days){
    return_today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings * (1 + return_today)
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    
    # Rebalancing
    holdings = total_wealth * weights
  }
  
  wealthtracker
}


#Profit/loss
mean(sim2[,n_days])
mean(sim2[,n_days] - total_wealth)

conf_5Per = confint(sim2[,n_days]- total_wealth, level = 0.90)$'5%'
cat('\nAverage return of investement after 20 days', mean(sim2[,n_days]), "\n")
cat('\n5% Value at Risk for high risk portfolio-',conf_5Per, "\n")

wealth_daywise = c()
  
for (i in 1:n_days){
    wealth_daywise[i] = mean(sim2[,i]) 
}
days = 1:n_days
df = data.frame(wealth_daywise, days)
ggplot(data=df, aes(x=days, y=wealth_daywise, group=1)) +
  geom_line(color="red")+
  geom_point() +
  xlab('Days') +
  ylab('Return of investments') + 
  ggtitle('High Risk Portfolio: Retruns over 20 days')
```

The last portfolio is a safe portfolio in which all low risk stocks are collectively given higher weights.

```{r , echo=FALSE, message=FALSE}

total_wealth = 100000

sim3 = foreach(i=1:5000, .combine = rbind) %do% {
  weights = c(0.3,0.03,0.32,0.35)
  holdings = total_wealth * weights
  n_days = 20
  wealthtracker = rep(0, n_days)
  
  for(today in 1:n_days){
    return_today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings * (1 + return_today)
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
    
    # Rebalancing
    holdings = total_wealth * weights
  }
  
  wealthtracker
}


#Profit/loss
mean(sim3[,n_days])
mean(sim3[,n_days] - total_wealth)

conf_5Per = confint(sim3[,n_days]- total_wealth, level = 0.90)$'5%'
cat('\nAverage return of investement after 20 days', mean(sim3[,n_days]), "\n")
cat('\n5% Value at Risk for low risk portfolio-',conf_5Per, "\n")

wealth_daywise = c()
  
for (i in 1:n_days){
    wealth_daywise[i] = mean(sim3[,i]) 
}
days = 1:n_days
df = data.frame(wealth_daywise, days)
ggplot(data=df, aes(x=days, y=wealth_daywise, group=1)) +
  geom_line(color="red")+
  geom_point() +
  xlab('Days') +
  ylab('Return of investments') + 
  ggtitle('Low Risk Portfolio: Retruns over 20 days')

````

**Conclusion**

Ironically, we see that the high risk portfolio is more attractive when it comes to the returns of investment but the normal portfolio has the best VaR.


# Market segmentation

Consider the data in social_marketing.csv. This was data collected in the course of a market-research study using followers of the Twitter account of a large consumer brand that shall remain nameless---let's call it "NutrientH20" just to have a label. The goal here was for NutrientH20 to understand its social-media audience a little bit better, so that it could hone its messaging a little more sharply.

Your task to is analyze this data as you see fit, and to prepare a concise report for NutrientH20 that identifies any interesting market segments that appear to stand out in their social-media audience. 


```{r , echo=FALSE, message=FALSE, warning=FALSE, comment=NA}
library (readr)
library (ggplot2)
library (corrplot)

urlfile="https://raw.githubusercontent.com/jgscott/STA380/master/data/social_marketing.csv"

mydata2<-read_csv(url(urlfile),col_types = cols())
head(mydata2)

cormat <- round(cor(mydata2[,2:37]), 2)
corrplot(cormat, method="circle")
```

**Observations:**

A lot of variables are correlated such as personal fitness and health nutrition which are strongly correlated.

```{r , echo=FALSE, message=FALSE, warning=FALSE}

mydata2$chatter<- NULL
mydata2$spam <- NULL
mydata2$adult <- NULL
mydata2$photo_sharing <- NULL 
mydata2$health_nutrition <- NULL 

pca_sm = prcomp(mydata2[,2:32], scale=TRUE, center = TRUE)

pca_var <-  pca_sm$sdev ^ 2
pca_var1 <- pca_var / sum(pca_var)

plot(cumsum(pca_var1), xlab = "Principal Component", 
     ylab = "Fraction of variance explained")

cumsum(pca_var1)[15]
```

**Observations:**

At 15th PC, around 77.62% of the variation is explained. 

**Let us try K Means**

```{r , echo=FALSE, message=FALSE, warning=FALSE}
library(cluster)
library(HSAUR)
library(fpc)
scores = pca_sm$x
pc_data <- as.data.frame(scores[,1:18])
X <- pc_data
X = scale(X, center=TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

library(LICORS)
library(factoextra)
fviz_nbclust(X, kmeans, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
clust1 = kmeanspp(X, 4,nstart=15)

social_clust1 <- cbind(mydata2, clust1$cluster)
plotcluster(mydata2[,2:32], clust1$cluster)
```

*We can see via the Elbow Method for K Means above that 4 is the optimal number of clusters.*

The clusters seem to be well separated now.

```{r , echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(ggthemes)
library(reshape2)
library(RCurl)
library(foreach)
library(fpc)
library(cluster)

social_clust1_main <- as.data.frame(cbind(clust1$center[1,]*sigma + mu, 
                            clust1$center[2,]*sigma + mu,
                            clust1$center[3,]*sigma + mu,
                            clust1$center[4,]*sigma + mu))
summary(social_clust1_main)
#Change column names
names(social_clust1_main) <- c("Cluster_1",
                "Cluster_2",
                "Cluster_3",
                "Cluster_4")
```

```{r , echo=FALSE}

social_clust1_main$type <- row.names(social_clust1_main)
#Cluster 1
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_1) , y=Cluster_1)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 1",
        x ="Category", y = "Cluster centre values") 
#cluster 2 
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_2) , y=Cluster_2)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 2",
        x ="Category", y = "Cluster centre values")
#Cluster 3
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_3) , y=Cluster_3)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 3",
        x ="Category", y = "Cluster centre values")
#Cluster 4
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_4) , y=Cluster_4)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 4",
        x ="Category", y = "Cluster centre values")


```

*I am not able to get the individual names for the PC's.*

**Hierarchial Clustering**

```{r , echo=FALSE}

distance_matrix = dist(X, method="euclidean")
hier = hclust(distance_matrix, method="complete")
cluster1 = cutree(hier, k=4)
summary(factor(cluster1))

social_clust1 <- cbind(mydata2, cluster1)

hcluster_average <- aggregate(social_clust1, list(social_clust1$cluster1), mean)
hcluster_average$cluster1 <- paste("Cluster_", hcluster_average$cluster1, sep = '')
hcluster_average$Group.1 <- NULL
hcluster_average$X <- NULL

row.names(hcluster_average) <- hcluster_average$cluster1
hcluster_average$cluster1 <- NULL
hcluster_average <- as.data.frame(t(hcluster_average))

hcluster_average$type <- row.names(hcluster_average)
social_clust1_main <- hcluster_average

#Cluster 1
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_1) , y=Cluster_1)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 1",
        x ="Category", y = "Cluster centre values")
#cluster 2 
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_2) , y=Cluster_2)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 2",
        x ="Category", y = "Cluster centre values")
#Cluster 3
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_3) , y=Cluster_3)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 3",
        x ="Category", y = "Cluster centre values")
#Cluster 4
ggplot(social_clust1_main, aes(x =reorder(type, -Cluster_4) , y=Cluster_4)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 4",
        x ="Category", y = "Cluster centre values")

```
Market segments within Hierarchical:

1. Sports_fandom, personal fitness, college_uni
2. Art, TV_film, travel
3. Cooking, politics, fashion
4. Dating, fashion, school

We can see that the demographics vary over the clusters. Cluster 4 is catered more towards young teenagers in school while Cluster 1 is consists of young adults who are into fitness and sports. Cluster 2 consists of people who are inclined to arts. Cluster 3 is very diverse as there are people interested in politics whilst enjoying cooking and fashion.

Market Segmentation is useful as it helps us to understand and send the message to the right group of people who will generate profits for the company.

# Author attribution

Revisit the Reuters C50 corpus that we explored in class. Your task is to build the best model you can, using any combination of tools you see fit, for predicting the author of an article on the basis of that article's textual content. Describe clearly what models you are using, how you constructed features, and so forth. Yes, this is a supervised learning task, but it potentially draws on a lot of what you know about unsupervised learning, since constructing features for a document might involve dimensionality reduction.


First, we'll load the necessary packages for text analysis and read the files.

````{r , message=FALSE, warning=FALSE}

library(tm) 
library(magrittr)
library(slam)
library(proxy)
library(caret)
library(plyr)
library(dplyr)
library(ggplot2)

readerP = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }

train=Sys.glob('C:/Users/venka/Downloads/STA380-master/STA380-master/data/ReutersC50/C50train/*')

```

We do pre-processing and tokenization by:
a) converting alphabet to lower case
b) removing numbers
c) removing punctuation
d) removing excess space
e) removing stop words

````{r, echo = FALSE,message=FALSE, warning=FALSE}

c_art=NULL
labels=NULL

for (name in train)
{ 
  author=substring(name,first=50)#first= ; ensure less than string length
  article=Sys.glob(paste0(name,'/*.txt'))
  c_art=append(c_art,article)
  labels=append(labels,rep(author,length(article)))
}
  
  readerPlain <- function(fname)
  {
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') 
  }
co = lapply(c_art, readerPlain) 
names(co) = c_art
names(co) = sub('.txt', '', names(co))
```

````{r , echo = FALSE,message=FALSE, warning=FALSE}
corp_tr=VCorpus(VectorSource(co))

corp_tr_cp=corp_tr 
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(tolower)) 
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(removeNumbers)) 
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(removePunctuation)) 
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(stripWhitespace))
corp_tr_cp = tm_map(corp_tr_cp, content_transformer(removeWords),stopwords("en")) 
DTM_train = DocumentTermMatrix(corp_tr_cp)
DTM_train 

DTM_tr=removeSparseTerms(DTM_train,.99)
tf_idf_mat = weightTfIdf(DTM_tr)
DTM_trr<-as.matrix(tf_idf_mat) #Matrix
tf_idf_mat
```

Now repeat the same as above for the test directories.

```{r , echo = FALSE, message=FALSE, warning=FALSE}
test=Sys.glob('C:/Users/venka/Downloads/STA380-master/STA380-master/data/ReutersC50/C50test/*')

c_art1=NULL
labels1=NULL

for (name in test)
{ 
  author1=substring(name,first=50)#first= ; ensure less than string length
  article1=Sys.glob(paste0(name,'/*.txt'))
  c_art1=append(c_art1,article1)
  labels1=append(labels1,rep(author1,length(article1)))
}

co1 = lapply(c_art1, readerPlain) 
names(co1) = c_art1
names(co1) = sub('.txt', '', names(co1))

corp_ts=VCorpus(VectorSource(co1))
```

```{r , echo=FALSE, message=FALSE, warning=FALSE}

corp_ts_cp=corp_ts #copy of the corp_tr file
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(tolower)) #convert to lower case
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(removeNumbers)) #remove numbers
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(removePunctuation)) #remove punctuation
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(stripWhitespace)) #remove excess space
corp_ts_cp = tm_map(corp_ts_cp, content_transformer(removeWords),stopwords("en")) #removing stopwords. Not exploring much on this, to avoid losing out on valuable information. 

#Ensuring same number of variables in test and train by specifying column names from the train document term matrix
DTM_ts=DocumentTermMatrix(corp_ts_cp,list(dictionary=colnames(DTM_tr)))
tf_idf_mat_ts = weightTfIdf(DTM_ts)
DTM_tss<-as.matrix(tf_idf_mat_ts) #Matrix
tf_idf_mat_ts #3394 words, 2500 documents
````

We perform PCA to extract the relevant features from many variables while not losing out on any important information.

We first remove the null columns.
```{r}
DTM_trr_1<-DTM_trr[,which(colSums(DTM_trr) != 0)] 
DTM_tss_1<-DTM_tss[,which(colSums(DTM_tss) != 0)]
```

We use only the intersecting columns.
```{r}
DTM_tss_1 = DTM_tss_1[,intersect(colnames(DTM_tss_1),colnames(DTM_trr_1))]
DTM_trr_1 = DTM_trr_1[,intersect(colnames(DTM_tss_1),colnames(DTM_trr_1))]
```

Extract the principal components
```{r}
mod_pca = prcomp(DTM_trr_1,scale=TRUE)
pred_pca=predict(mod_pca,newdata = DTM_tss_1)
````

Choosing the number of principal components
```{r, echo=FALSE}
plot(mod_pca,type='line') 
var <- apply(mod_pca$x, 2, var)  
prop <- var / sum(var)
plot(cumsum(mod_pca$sdev^2/sum(mod_pca$sdev^2)))
````

We prepare the dataset to be used in the classification models.
```{r}
tr_class = data.frame(mod_pca$x[,1:725])
tr_class['author']=labels
tr_load = mod_pca$rotation[,1:725]
ts_class_pre <- scale(DTM_tss_1) %*% tr_load
ts_class <- as.data.frame(ts_class_pre)
ts_class['author']=labels1
````

We now try the classification models for author attribution

A) Random Forest

```{r , echo=FALSE, message=FALSE, warning=FALSE}
library(randomForest)
set.seed(123)
mod_rand<-randomForest(as.factor(author)~.,data=tr_class, mtry=6,importance=TRUE)

pre_rand<-predict(mod_rand,data=ts_class)
tab_rand<-as.data.frame(table(pre_rand,as.factor(ts_class$author)))
predicted<-pre_rand
actual<-as.factor(ts_class$author)
temp<-as.data.frame(cbind(actual,predicted))
temp$flag<-ifelse(temp$actual==temp$predicted,1,0)
sum(temp$flag)
sum(temp$flag)*100/nrow(temp)
````

B) Naive Bayes
```{r , echo=FALSE, message=FALSE, warning=FALSE}
library('e1071')
mod_naive=naiveBayes(as.factor(author)~.,data=tr_class)
pred_naive=predict(mod_naive,ts_class)

predicted_nb=pred_naive
actual_nb=as.factor(ts_class$author)

temp_nb<-as.data.frame(cbind(actual_nb,predicted_nb))
temp_nb$flag<-ifelse(temp_nb$actual_nb==temp_nb$predicted_nb,1,0)
sum(temp_nb$flag)
sum(temp_nb$flag)*100/nrow(temp_nb)

pred_naive_tr=predict(mod_naive,tr_class)
tr_err_naive_pre<-pred_naive
````

C) KNN
```{r , echo=FALSE, message=FALSE, warning=FALSE}
train.X = subset(tr_class, select = -c(author))
test.X = subset(ts_class,select=-c(author))
train.author=as.factor(tr_class$author)
test.author=as.factor(ts_class$author)

library(class)
set.seed(111)
knn_pred=knn(train.X,test.X,train.author,k=1)

temp_knn=as.data.frame(cbind(knn_pred,test.author))
temp_knn_flag<-ifelse(as.integer(knn_pred)==as.integer(test.author),1,0)
sum(temp_knn_flag)
sum(temp_knn_flag)*100/nrow(temp_knn)
````

Let us compare the accuracies which were 74.68 for Random Forest, 31.16 for Naive Bayes and 33.2 for KNN. 
```{r , echo=FALSE}
library(ggplot2)
comp<-data.frame("Model"=c("Random Forest","Naive Baye's","KNN"), "Test.accuracy"=c(74.68,31.16,33.2))
comp
ggplot(comp,aes(x=Model,y=Test.accuracy))+geom_col()
```


# Association rule mining

Use the data on grocery purchases in groceries.txt and find some interesting association rules for these shopping baskets. Pick your own thresholds for lift and confidence; just be clear what these thresholds are and how you picked them. Do your discovered item sets make sense? Present your discoveries in an interesting and concise way.

Present the structure of the raw dataset

```{r, echo=FALSE, message=FALSE, warning=FALSE}
setwd("~/ML")
groceries = scan("groceries.txt", what = "", sep = "\n")
str(groceries)

# Loading package
library(arules)
library(arulesViz)
groceries = strsplit(groceries, ",")
trans <- as(groceries, "transactions")
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
itemFrequencyPlot(trans, topN = 15)
```
 We see that whole milk is the most frequently bought item.
 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
associa_rules = apriori(data = trans, 
                        parameter = list(support=0.005, confidence=.1, minlen=2))
length(associa_rules)
inspect(associa_rules[1:10])
````
There are initially 1575 rules when we take support as 0.005 and confidence as 0.1.
Whole milk is in most of the relationships.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
subset.rules <- which(colSums(is.subset(associa_rules, associa_rules)) > 1) # get subset rules in vector
length(subset.rules)  
subset.association.rules <- associa_rules[-subset.rules]
inspect(subset.association.rules)
````

There are now 1161 rules. We will draw the necessary graph.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
inspect(sort(subset.association.rules, by = 'lift'))
plot(subset.association.rules, method = "graph", 
     measure = "confidence", shading = "lift")
````

Now, let decrease the support to 0.005 and increase confidence to 0.4.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
associa_rules1 = apriori(data = trans, 
                        parameter = list(support=0.005, confidence=.4, minlen=2))
length(associa_rules1)
inspect(associa_rules1)
```
 
There are only 6 rules now. Let us draw the respective graph.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
inspect(sort(associa_rules1, by = 'lift'))
plot(associa_rules1, method = "graph", 
     measure = "confidence", shading = "lift")
````


Now, we'll decrease support to 0.001 and increase confidence to 0.8.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
associa_rules2 = apriori(data = trans, 
                        parameter = list(support=0.001, confidence=.8, minlen=2))

length(associa_rules2)
inspect(associa_rules2[1:10])

plot(head(associa_rules2, 5, by='lift'), method='graph')
````
 There are 410 rules and we saw whole milk mostly when we took the first five. 
 
Observations:

1. Whole milk is more frequently purchased.
2. People opt for vegetables when they buy juices.



 